{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List, Union\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import zscore\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA, IncrementalPCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from pickle import dump, load\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PcaDetector(object):\n",
    "    def __init__(self, model_directory: str, model_file_name: str):\n",
    "        \"\"\"\n",
    "        PCA 모델을 초기화하거나 새롭게 생성하는 클래스\n",
    "        model_directory (string): 모델이 저장된 디렉터리 경로\n",
    "        model_file_name (string): 모델 파일 이름\n",
    "        \"\"\"\n",
    "        self.model = self._get_pca_model(model_directory, model_file_name)\n",
    "        self.labels = {0: \"Normal\", 1: \"Abnormal\"}\n",
    "        self.cov_matrix = None\n",
    "        self.inv_cov_matrix = None\n",
    "        self.mean_distr = None\n",
    "        self.prior_mean = None\n",
    "        self.prior_var = None\n",
    "        self.update_threshold = 10.0  # 업데이트 여부를 판단할 임계값 설정\n",
    "\n",
    "    def _get_pca_model(self, model_directory: str, model_file_name: str):\n",
    "        model_file_directory = os.path.join(model_directory, model_file_name)\n",
    "        try:\n",
    "            with open(model_file_directory, \"rb\") as f:\n",
    "                model = load(f)\n",
    "        except FileNotFoundError as e:\n",
    "            print(f\"Model file cannot be found: {e}\")\n",
    "            print(\"Declare New Model Pipeline with Incremental PCA\")\n",
    "            # Incremental PCA로 새로운 모델을 생성\n",
    "            model = Pipeline(\n",
    "                steps=[\n",
    "                    (\"scaler\", StandardScaler()),  # 스케일러로 입력 데이터 표준화\n",
    "                    (\"ipca\", IncrementalPCA(n_components=2)),  # Incremental PCA 사용\n",
    "                ]\n",
    "            )\n",
    "        return model\n",
    "\n",
    "    def _should_update(self, data: np.ndarray) -> bool:\n",
    "        \"\"\"\n",
    "        새로운 데이터가 기존 데이터와 너무 다르면 업데이트를 하지 않음\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.inv_cov_matrix is None or self.mean_distr is None:\n",
    "            return True  # 모델이 초기화되지 않았으면 업데이트 허용\n",
    "        \n",
    "        # Step 2: 데이터를 PCA로 변환\n",
    "        x_scaled = self.model.named_steps[\"scaler\"].transform(data)\n",
    "        x_pca = self.model.named_steps[\"ipca\"].transform(x_scaled)\n",
    "        \n",
    "        mahalanobis_distances = self._calculate_MahalanobisDist(\n",
    "            self.inv_cov_matrix, self.mean_distr, x_pca\n",
    "        )\n",
    "        # 마할라노비스 거리가 임계값을 넘으면 업데이트하지 않음\n",
    "        if np.mean(mahalanobis_distances) > self.update_threshold:\n",
    "            print(\"New data significantly deviates from the existing data. Skipping update.\")\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    def _update(self, x: Union[pd.DataFrame, np.ndarray], scale: bool = False) -> None:\n",
    "        if not self._should_update(x):\n",
    "            return  # 업데이트하지 않음\n",
    "\n",
    "        if scale:\n",
    "            # 매번 fit 대신 partial_fit으로 이전 데이터를 유지한 채 업데이트\n",
    "            self.model.named_steps[\"scaler\"].partial_fit(x)\n",
    "\n",
    "        x_scaled = self.model.named_steps[\"scaler\"].transform(x)\n",
    "        # Incremental PCA를 통해 점진적으로 학습\n",
    "        self.model.named_steps[\"ipca\"].partial_fit(x_scaled)\n",
    "\n",
    "        # PCA 변환 결과를 얻음\n",
    "        x_pca = self.model.named_steps[\"ipca\"].transform(x_scaled)\n",
    "\n",
    "        # 공분산 행렬 및 평균 벡터 업데이트\n",
    "        self.cov_matrix, self.inv_cov_matrix = self._get_cov_matrix(x_pca)\n",
    "        self.mean_distr = np.mean(x_pca, axis=0)\n",
    "\n",
    "    def _get_cov_matrix(self, data: Union[pd.DataFrame, np.ndarray]):\n",
    "        covariance_matrix = np.cov(data, rowvar=False)\n",
    "        inv_cov_matrix = np.linalg.inv(covariance_matrix)\n",
    "        return covariance_matrix, inv_cov_matrix\n",
    "\n",
    "    def _calculate_MahalanobisDist(\n",
    "        self, inv_cov_matrix: np.ndarray, mean_distr: np.ndarray, data: np.ndarray\n",
    "    ):\n",
    "        \"\"\"\n",
    "        PCA로 변환된 데이터와 평균 벡터를 사용하여 마할라노비스 거리 계산\n",
    "        \"\"\"\n",
    "        diff = data - mean_distr\n",
    "        md = []\n",
    "        for i in range(len(diff)):\n",
    "            md.append(np.sqrt(diff[i].dot(inv_cov_matrix).dot(diff[i])))\n",
    "        return md\n",
    "\n",
    "    def predict(self, dist: np.ndarray, extreme=True) -> np.ndarray:\n",
    "        # 베이지안 방식으로 임계값 설정\n",
    "       \n",
    "        threshold = self._MD_threshold(dist, threshold, extreme)\n",
    "        outliers = []\n",
    "        for i in range(len(dist)):\n",
    "            if dist[i] >= threshold:\n",
    "                outliers.append(i)  # index of the outlier\n",
    "        return np.array(outliers)\n",
    "\n",
    "    def _MD_threshold(self, dist: np.ndarray, threshold=3.0, extreme=False):\n",
    "        \"\"\"\n",
    "        베이지안 방식으로 임계값을 업데이트하는 함수\n",
    "        \"\"\"\n",
    "        # 새로운 데이터의 평균과 분산\n",
    "        new_mean = np.mean(dist)\n",
    "        new_var = np.var(dist)\n",
    "\n",
    "        # 이전 평균과 분산이 없는 경우 초기화\n",
    "        if self.prior_mean is None or self.prior_var is None:\n",
    "            updated_mean = new_mean  # 새로운 데이터의 평균을 업데이트된 값으로 설정\n",
    "            updated_var = new_var  # 새로운 데이터의 분산을 업데이트된 값으로 설정\n",
    "            self.prior_mean = updated_mean  # 이후 참조를 위해 저장\n",
    "            self.prior_var = updated_var  # 이후 참조를 위해 저장\n",
    "        else:\n",
    "            # 가중치를 적용하여 업데이트 (이전 데이터의 크기를 고려)\n",
    "            weight_prior = 0.8  # 이전 평균에 80% 가중치\n",
    "            weight_new = 0.2    # 새로운 평균에 20% 가중치\n",
    "            updated_mean = (weight_prior * self.prior_mean) + (weight_new * new_mean)\n",
    "            updated_var = (weight_prior * self.prior_var) + (weight_new * new_var)\n",
    "\n",
    "            # 이전 상태 업데이트\n",
    "            self.prior_mean = updated_mean\n",
    "            self.prior_var = updated_var\n",
    "\n",
    "        # 업데이트된 임계값 계산\n",
    "        k = threshold if extreme else threshold - 1\n",
    "        dynamic_threshold = updated_mean + k * np.sqrt(updated_var)\n",
    "\n",
    "        return dynamic_threshold\n",
    "\n",
    "    def process_and_detect(self, data: Union[pd.DataFrame, np.ndarray], extreme=True) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        새로운 데이터를 처리하고 PCA 모델을 업데이트한 뒤 이상 탐지 수행\n",
    "        \"\"\"\n",
    "        # Step 1: PCA 모델 업데이트\n",
    "        if not self.inv_cov_matrix and not self.cov_matrix:\n",
    "            self._update(data, scale=True)\n",
    "\n",
    "        # Step 2: 데이터를 PCA로 변환\n",
    "        x_scaled = self.model.named_steps[\"scaler\"].transform(data)\n",
    "        x_pca = self.model.named_steps[\"ipca\"].transform(x_scaled)\n",
    "\n",
    "        # Step 3: 마할라노비스 거리 계산\n",
    "        mahalanobis_distances = self._calculate_MahalanobisDist(\n",
    "            self.inv_cov_matrix, self.mean_distr, x_pca  # 변환된 PCA 데이터 사용\n",
    "        )\n",
    "\n",
    "        # Step 4: 이상치 탐지\n",
    "        outliers = self.predict(mahalanobis_distances, extreme=extreme)\n",
    "        self._upate()\n",
    "        return outliers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 데이터 생성\n",
    "np.random.seed(42)\n",
    "normal_data = np.random.normal(loc=0, scale=1, size=(100, 5))\n",
    "abnormal_data = np.random.normal(loc=5, scale=1, size=(20, 5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model file cannot be found: [Errno 2] No such file or directory: 'models/pca_model.pkl'\n",
      "Declare New Model Pipeline with Incremental PCA\n"
     ]
    }
   ],
   "source": [
    "# PCA 기반 이상 탐지 모델 초기화 (처음에 PCA 모델이 없다고 가정)\n",
    "pca_detector = PcaDetector(model_directory=\"models\", model_file_name=\"pca_model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1: Detected outliers: [False False False False False False False False False False]\n",
      "Batch 2: Detected outliers: [False False False False False False False False False False]\n",
      "Batch 3: Detected outliers: [ True  True False False False False  True  True  True False]\n",
      "Batch 4: Detected outliers: [False False False  True False  True False False False False]\n",
      "Batch 5: Detected outliers: [ True  True  True False  True False  True False False False]\n",
      "Batch 6: Detected outliers: [ True False  True  True False False False False False False]\n",
      "Batch 7: Detected outliers: [False False False False False False False False False  True]\n",
      "Batch 8: Detected outliers: [False False False False False False  True  True  True False]\n",
      "Batch 9: Detected outliers: [False False False False False  True False False False False]\n",
      "Batch 10: Detected outliers: [False False False False False  True False False False False]\n"
     ]
    }
   ],
   "source": [
    "# 정상 및 비정상 데이터를 점진적으로 처리하며 이상 탐지 수행\n",
    "new_threshold = 3.0\n",
    "train = True\n",
    "for i in range(0, len(normal_data), 10):  # 데이터를 10개씩 묶어 처리\n",
    "    batch = normal_data[i:i+10]\n",
    "    if train:\n",
    "        pca_detector._update(batch, scale=True)\n",
    "        train = False\n",
    "    scaled_batch = pca_detector.model.named_steps[\"scaler\"].transform(batch)\n",
    "    x_pca_batch = pca_detector.model.named_steps[\"ipca\"].transform(scaled_batch)\n",
    "    # cov_matrix, inv_cov_matrix = pca_detector._get_cov_matrix(x_pca_batch)\n",
    "    # mean_distr = x_pca_batch.mean(axis=0)\n",
    "    dist_train = pca_detector._calculate_MahalanobisDist(pca_detector.inv_cov_matrix, pca_detector.mean_distr, x_pca_batch)\n",
    "    new_threshold = pca_detector._MD_threshold(dist_train, threshold=new_threshold, extreme=False)\n",
    "    outliers = np.array(dist_train >= new_threshold)\n",
    "    pca_detector._update(batch, scale=True)\n",
    "    print(f\"Batch {i//10 + 1}: Detected outliers:\", outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected outliers in abnormal data batch: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19]\n"
     ]
    }
   ],
   "source": [
    "anomaly_batch = abnormal_data \n",
    "\n",
    "scaled_batch = pca_detector.model.named_steps[\"scaler\"].transform(anomaly_batch)\n",
    "x_pca_batch = pca_detector.model.named_steps[\"ipca\"].transform(scaled_batch)\n",
    "\n",
    "dist_train = pca_detector._calculate_MahalanobisDist(pca_detector.inv_cov_matrix, pca_detector.mean_distr, x_pca_batch)\n",
    "new_threshold = pca_detector._MD_threshold(dist_train, threshold=new_threshold, extreme=True)\n",
    "\n",
    "outliers = []\n",
    "for i in range(len(dist_train)):\n",
    "    if dist_train[i] >= new_threshold:\n",
    "        outliers.append(i)  # index of the outlier\n",
    "        \n",
    "outliers = np.array(outliers)\n",
    "# outliers = np.array(dist_train >= new_threshold)\n",
    "# pca_detector._update(abnormal_data, scale=True)\n",
    "print(\"Detected outliers in abnormal data batch:\", outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected outliers in abnormal data batch: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19]\n"
     ]
    }
   ],
   "source": [
    "outliers = np.where(dist_train >= new_threshold)[0]  # 조건을 만족하는 인덱스 반환\n",
    "print(\"Detected outliers in abnormal data batch:\", outliers)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-pipe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
